# Gu√≠a Paso a Paso: Configuraci√≥n desde Cero

## üéØ Objetivo
Configurar Confluent Cloud con AWS para entender y usar **TOPICS** en streaming de datos.

## üìö Investigaci√≥n sobre TOPICS

### ¬øQu√© son los Topics?
Los **Topics** en Confluent/Kafka son contenedores l√≥gicos donde se almacenan los datos. Piensa en ellos como:
- **Carpetas** donde organizas diferentes tipos de mensajes
- **Canales de comunicaci√≥n** entre aplicaciones
- **Flujos de datos** espec√≠ficos por categor√≠a

### Ejemplos de Topics para tu Proyecto
1. `estudiante-actividades` - Actividades de estudiantes
2. `sistema-notificaciones` - Notificaciones del sistema
3. `logs-aplicacion` - Logs de la aplicaci√≥n
4. `eventos-usuario` - Eventos de interacci√≥n del usuario

---

## üöÄ PARTE 1: Configurar Confluent Cloud

### Paso 1: Crear tu Primer Cluster
1. Ve a https://confluent.cloud
2. Haz login con tu cuenta
3. Busca el bot√≥n **"Create cluster"** o **"Crear cluster"**
4. Selecciona:
   - **Tipo**: Basic (gratis)
   - **Regi√≥n**: us-east-1 (misma que AWS)
   - **Nombre**: `mi-primer-cluster`

### Paso 2: Generar API Keys
1. Una vez creado el cluster, ve a **"API Keys"**
2. Haz clic en **"Create key"** o **"+ Add key"**
3. Selecciona **"Global access"**
4. **‚ö†Ô∏è IMPORTANTE**: Copia y guarda:
   - API Key
   - API Secret
   (Los necesitar√°s para el c√≥digo)

### Paso 3: Crear Topics
1. En tu cluster, ve a **"Topics"**
2. Haz clic en **"Create topic"** o **"+ Add topic"**
3. Crea estos topics:
   - Nombre: `estudiante-eventos`
   - Particiones: 1 (para empezar)
   - Retention: 7 days

4. Crea otro topic:
   - Nombre: `sistema-logs`
   - Particiones: 1
   - Retention: 7 days

---

## ‚òÅÔ∏è PARTE 2: Configurar AWS

### Paso 1: Crear Usuario IAM
1. Ve a AWS Console ‚Üí IAM
2. Usuarios ‚Üí **"Agregar usuario"**
3. Nombre: `confluent-integration-user`
4. Tipo de acceso: **"Acceso mediante programaci√≥n"**
5. Permisos: Agregar pol√≠ticas existentes:
   - `AmazonS3FullAccess`
   - `CloudWatchFullAccess`
6. **‚ö†Ô∏è IMPORTANTE**: Guarda:
   - Access Key ID
   - Secret Access Key

### Paso 2: Crear Bucket S3
1. Ve a AWS Console ‚Üí S3
2. **"Crear bucket"**
3. Nombre: `mi-proyecto-confluent-[tu-nombre]` (debe ser √∫nico)
4. Regi√≥n: us-east-1
5. Configuraci√≥n por defecto

### Paso 3: Crear Log Group en CloudWatch
1. Ve a AWS Console ‚Üí CloudWatch
2. Logs ‚Üí Log groups
3. **"Crear grupo de registros"**
4. Nombre: `/confluent/mi-aplicacion`

---

## üíª PARTE 3: Configurar el Proyecto

### Paso 1: Instalar Dependencias
```powershell
# Ejecutar en PowerShell dentro de tu carpeta del proyecto
pip install kafka-python boto3 python-dotenv
```

### Paso 2: Configurar Variables de Entorno
1. Copia el archivo `config/.env.example` a `config/.env`
2. Edita `config/.env` con tus datos:

```env
# Confluent Cloud (datos de tu cluster)
CONFLUENT_BOOTSTRAP_SERVERS=pkc-xxxxx.us-east-1.aws.confluent.cloud:9092
CONFLUENT_API_KEY=tu-api-key-aqui
CONFLUENT_API_SECRET=tu-api-secret-aqui

# AWS (datos de tu usuario IAM)
AWS_ACCESS_KEY_ID=tu-access-key-aqui
AWS_SECRET_ACCESS_KEY=tu-secret-key-aqui
AWS_REGION=us-east-1
AWS_S3_BUCKET=mi-proyecto-confluent-[tu-nombre]
AWS_CLOUDWATCH_LOG_GROUP=/confluent/mi-aplicacion

# Topics que creaste
TOPIC_USER_EVENTS=estudiante-eventos
TOPIC_SYSTEM_LOGS=sistema-logs
TOPIC_NOTIFICATIONS=notificaciones
```

---

## üß™ PARTE 4: Probar la Integraci√≥n

### Paso 1: Probar Conexiones
```powershell
# Ejecutar el script de pruebas
python test_integration.py
```

### Paso 2: Enviar Datos de Prueba
```powershell
# Ejecutar el producer
python producers/basic_producer.py
```

### Paso 3: Consumir Datos
```powershell
# En otra terminal, ejecutar el consumer
python consumers/integrated_consumer.py
```

---

## üìä ¬øQu√© va a pasar?

1. **Producer** enviar√° mensajes a los topics en Confluent Cloud
2. **Consumer** leer√° esos mensajes y los enviar√° a AWS
3. **S3** almacenar√° los datos para an√°lisis posterior
4. **CloudWatch** mostrar√° logs y m√©tricas

---

## üîç Para tu Investigaci√≥n sobre Topics

### Preguntas que puedes responder:
1. **¬øC√≥mo se organizan los datos en un topic?**
2. **¬øQu√© ventajas tienen las particiones?**
3. **¬øC√≥mo garantizan el orden de los mensajes?**
4. **¬øQu√© pasa si un consumer se desconecta?**
5. **¬øC√≥mo se escalan los topics?**

### Experimentos que puedes hacer:
1. Crear diferentes tipos de topics
2. Enviar mensajes con diferentes estructuras
3. Probar m√∫ltiples consumers
4. Analizar el rendimiento con muchos mensajes

---

## üìù Notas Importantes

- **Topics son persistentes**: Los datos no se pierden al desconectarse
- **Escalabilidad**: Puedes agregar m√°s particiones despu√©s
- **Orden**: Solo se garantiza dentro de cada partici√≥n
- **Consumers**: M√∫ltiples aplicaciones pueden leer el mismo topic
- **Retention**: Los datos se mantienen por el tiempo configurado

---

## üÜò Si algo no funciona:

1. Verifica que las credenciales est√©n correctas en `.env`
2. Aseg√∫rate de que los topics existan en Confluent Cloud
3. Revisa que el bucket S3 y CloudWatch est√©n creados
4. Ejecuta `python test_integration.py` para diagnosticar problemas
