"""
Consumer integrado que procesa mensajes y los env√≠a a AWS
Este script combina el consumo de Confluent Cloud con la integraci√≥n a AWS
"""

import json
import os
import sys
sys.path.append('../aws-integration')

from kafka import KafkaConsumer
from dotenv import load_dotenv
from aws_services import AWSIntegration
import logging
import signal

# Cargar variables de entorno
load_dotenv(dotenv_path='../config/.env')

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntegratedConsumer:
    def __init__(self, topics: list, group_id: str = "aws-integration-group"):
        """
        Inicializa el consumer integrado con AWS
        
        Args:
            topics (list): Lista de topics a consumir
            group_id (str): ID del grupo de consumidores
        """
        self.bootstrap_servers = os.getenv('CONFLUENT_BOOTSTRAP_SERVERS')
        self.api_key = os.getenv('CONFLUENT_API_KEY')
        self.api_secret = os.getenv('CONFLUENT_API_SECRET')
        
        if not all([self.bootstrap_servers, self.api_key, self.api_secret]):
            raise ValueError("Faltan credenciales de Confluent Cloud en .env")
        
        # Configurar consumer de Kafka
        self.consumer = KafkaConsumer(
            *topics,
            bootstrap_servers=self.bootstrap_servers,
            security_protocol='SASL_SSL',
            sasl_mechanism='PLAIN',
            sasl_plain_username=self.api_key,
            sasl_plain_password=self.api_secret,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            key_deserializer=lambda k: k.decode('utf-8') if k else None,
            group_id=group_id,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            auto_commit_interval_ms=1000
        )
        
        # Inicializar integraci√≥n AWS
        self.aws_integration = AWSIntegration()
        
        self.running = True
        self.message_count = 0
        
        logger.info(f"Consumer integrado configurado para topics: {topics}")
    
    def process_message(self, message):
        """
        Procesa un mensaje y lo env√≠a a AWS
        
        Args:
            message: Mensaje de Kafka
        """
        try:
            self.message_count += 1
            
            print(f"\nüì® Mensaje #{self.message_count} recibido:")
            print(f"   Topic: {message.topic}")
            print(f"   Key: {message.key}")
            print(f"   Timestamp: {message.timestamp}")
            print(f"   Value: {json.dumps(message.value, indent=2, ensure_ascii=False)}")
            
            # Procesar seg√∫n el topic
            if message.topic == 'user-events':
                self.process_user_event(message.value)
            elif message.topic == 'system-logs':
                self.process_system_log(message.value)
            elif message.topic == 'notifications':
                self.process_notification(message.value)
            else:
                # Procesamiento gen√©rico
                self.process_generic_message(message.topic, message.value)
            
            print("‚úÖ Mensaje procesado y enviado a AWS")
            print("-" * 60)
            
        except Exception as e:
            logger.error(f"Error procesando mensaje: {e}")
            print(f"‚ùå Error procesando mensaje: {e}")
    
    def process_user_event(self, event_data):
        """
        Procesa eventos de usuario y los env√≠a a AWS
        
        Args:
            event_data (dict): Datos del evento
        """
        print("üîµ Procesando evento de usuario...")
        
        # Enviar a AWS
        success = self.aws_integration.process_user_event_to_aws(event_data)
        
        if success:
            print("   ‚Üí Evento almacenado en S3")
            print("   ‚Üí Log enviado a CloudWatch")
            print("   ‚Üí M√©trica enviada a CloudWatch")
        
        # L√≥gica de negocio adicional
        event_type = event_data.get('event_type')
        user_id = event_data.get('user_id')
        
        if event_type == 'login':
            print(f"   ‚Üí Usuario {user_id} ha iniciado sesi√≥n")
        elif event_type == 'purchase':
            print(f"   ‚Üí Usuario {user_id} ha realizado una compra")
            # Aqu√≠ podr√≠as actualizar inventario, enviar confirmaci√≥n, etc.
        elif event_type == 'logout':
            print(f"   ‚Üí Usuario {user_id} ha cerrado sesi√≥n")
    
    def process_system_log(self, log_data):
        """
        Procesa logs del sistema y los env√≠a a AWS
        
        Args:
            log_data (dict): Datos del log
        """
        print("üìù Procesando log del sistema...")
        
        # Enviar a AWS
        success = self.aws_integration.process_system_log_to_aws(log_data)
        
        if success:
            print("   ‚Üí Log enviado a CloudWatch")
            if log_data.get('level') == 'ERROR':
                print("   ‚Üí Error almacenado en S3 para an√°lisis")
            print("   ‚Üí M√©trica enviada a CloudWatch")
        
        # L√≥gica de alerta para errores cr√≠ticos
        if log_data.get('level') == 'ERROR':
            print("   üö® ERROR CR√çTICO DETECTADO")
            # Aqu√≠ podr√≠as enviar alertas, notificaciones, etc.
    
    def process_notification(self, notification_data):
        """
        Procesa notificaciones
        
        Args:
            notification_data (dict): Datos de la notificaci√≥n
        """
        print("üîî Procesando notificaci√≥n...")
        
        # Almacenar en S3
        self.aws_integration.store_data_in_s3(
            notification_data, 
            f"notifications/{notification_data.get('recipient')}/notification.json"
        )
        
        # Enviar log
        log_message = f"NOTIFICATION: {notification_data.get('title')} to {notification_data.get('recipient')}"
        self.aws_integration.send_to_cloudwatch_logs(log_message, "notifications")
        
        # Enviar m√©trica
        self.aws_integration.send_metric_to_cloudwatch("NotificationsSent", 1)
        
        print("   ‚Üí Notificaci√≥n almacenada en S3")
        print("   ‚Üí Log enviado a CloudWatch")
        print("   ‚Üí M√©trica actualizada")
    
    def process_generic_message(self, topic, message_data):
        """
        Procesamiento gen√©rico para cualquier mensaje
        
        Args:
            topic (str): Nombre del topic
            message_data (dict): Datos del mensaje
        """
        print(f"üìÑ Procesando mensaje gen√©rico del topic: {topic}")
        
        # Almacenar en S3
        s3_key = f"generic-messages/{topic}/{message_data.get('timestamp', 'unknown')}.json"
        self.aws_integration.store_data_in_s3(message_data, s3_key)
        
        # Enviar m√©trica
        self.aws_integration.send_metric_to_cloudwatch(f"Messages_{topic}", 1)
        
        print("   ‚Üí Mensaje almacenado en S3")
        print("   ‚Üí M√©trica enviada a CloudWatch")
    
    def start_consuming(self):
        """Inicia el consumo de mensajes"""
        logger.info("Iniciando consumer integrado con AWS...")
        print("üöÄ Consumer AWS integrado iniciado")
        print("üì° Conectado a Confluent Cloud")
        print("‚òÅÔ∏è  Conectado a AWS")
        print("Presiona Ctrl+C para detener.")
        print("=" * 60)
        
        try:
            for message in self.consumer:
                if not self.running:
                    break
                
                self.process_message(message)
                
        except KeyboardInterrupt:
            logger.info("Recibida se√±al de interrupci√≥n")
        except Exception as e:
            logger.error(f"Error en el consumer: {e}")
        finally:
            self.stop()
    
    def stop(self):
        """Detiene el consumer"""
        self.running = False
        self.consumer.close()
        
        print(f"\nüìä Estad√≠sticas finales:")
        print(f"   Mensajes procesados: {self.message_count}")
        
        logger.info("Consumer integrado detenido")

def signal_handler(signum, frame):
    """Manejador de se√±ales para detener gracefully"""
    print("\nüõë Deteniendo consumer integrado...")
    global consumer
    if 'consumer' in globals():
        consumer.stop()
    sys.exit(0)

def main():
    """Funci√≥n principal"""
    global consumer
    
    # Configurar manejador de se√±ales
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Topics a consumir
    topics = [
        os.getenv('TOPIC_USER_EVENTS', 'user-events'),
        os.getenv('TOPIC_SYSTEM_LOGS', 'system-logs'),
        os.getenv('TOPIC_NOTIFICATIONS', 'notifications')
    ]
    
    try:
        # Crear y iniciar consumer
        consumer = IntegratedConsumer(topics, group_id="aws-integration-group")
        consumer.start_consuming()
        
    except Exception as e:
        logger.error(f"Error iniciando consumer: {e}")
        print(f"‚ùå Error: {e}")
        print("\nüí° Verifica que:")
        print("   1. El archivo .env est√© configurado correctamente")
        print("   2. Las credenciales de Confluent Cloud sean v√°lidas")
        print("   3. Las credenciales de AWS sean v√°lidas")
        print("   4. Los topics existan en Confluent Cloud")

if __name__ == "__main__":
    main()
